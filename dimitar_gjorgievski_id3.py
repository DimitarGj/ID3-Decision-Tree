# -*- coding: utf-8 -*-
"""Dimitar_Gjorgievski_ID3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-ogR2LMuxmsom2QLmK4hIhIh7s0Xx46
"""

#Add your name here
#Name: Dimitar Gjorgievski

"""First save a copy on this notebook in your Drive: File->Save a copy in Drive, then rename your file to firstname_lastname_id3.ipynb (e.g. pedram_rooshenas_id3.ipynb). Now you can start

Submission:

1- Run all cells (this is important, the results will remain there for us to look)

2- Download .ipynb

4- Submit .ipynb on Gradescope (After submission double-Check to make sure the cell outputs are there, otherwise you won't get any grade)
"""

import re
import pandas as pd
import math
import numpy as np

"""If you want to continue using Google Colab (strongly recommended), you can load the data to your Google Drive and mount it here

If you like to continue with your local machine, download the ID3.ipynb and run it on your local Jupyter.

"""

#Mounting Google Drive:
#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.
#You can either use your personal Google account or your UIC Google account.
from google.colab import drive
drive.mount('/content/gdrive')

#After successfull mount, you can browse your Google Drive using linux commands:
!ls /content/gdrive/

#I have placed the data under mlcourse/hw1/agaricus in my Google Drive, so the address is:
#Yours would be different!

!head /content/gdrive/MyDrive/cs412/ID3/agaricus/agaricuslepiotatest1.csv

#For this programs we are going to evaluate the ID3 on three different datasets:
#Change the path based on your directory address
path="/content/gdrive/MyDrive/cs412/ID3/"


#for agaricus we don't have a separate validation set, so we are going to reuse the training set.
agaricus = ["agaricus/agaricuslepiotatrain1.csv",
              "agaricus/agaricuslepiotatrain1.csv",
              "agaricus/agaricuslepiotatest1.csv"]

dataset1 = ["data_sets1/training_set.csv",
            "data_sets1/validation_set.csv",
            "data_sets1/test_set.csv"]

# Load data from a file. It returns a list of data points as well as the list of variable names
def read_data(filename):
    f = open(filename, 'r')
    p = re.compile(',')
    data = []
    header = f.readline().strip()
    varnames = p.split(header)
    namehash = {}
    for l in f:
        data.append([int(x) for x in p.split(l.strip())])
    return (data, varnames)

dataset = agaricus
train_data, varnames = read_data(path+dataset[0])
#the last element in the list is the class value."

#You can transfer the data to Pandas dataframe or directly load it with pd.read_csv but manupulating python lists would be easier.
#You can also use pandas to explore at the data.
data_df = pd.DataFrame(train_data, columns=varnames)
data_df

#We check the dimensions of our dataframe
data_df.shape

#The first number in the shape is the xnumber of rows and the second one is the number of columns
#so we can quickly find the number examples:
data_df.shape[0]

#The class label is often the last column
classlabel = data_df.columns[-1]
print(classlabel)

#e.g. number of instances with positive class
data_df['poisonous'].sum()

#First attribute:
attr1 = data_df.columns[0]
print(attr1)

#Finding the rows that cap-shape-bell is one:
data_df[data_df['cap-shape-bell'] == 1]

#Counting the number of rows that cap-shape-bell is one
(data_df['cap-shape-bell'] == 1).sum()

#Rows that have both  cap-shape-bell==1 and are population-several
data_df[(data_df['cap-shape-bell'] == 1) & (data_df['population-several'] == 1)]

#Number of rows:
((data_df['cap-shape-bell'] == 1) & (data_df['population-several'] == 1)).sum()

#Lets drop cap-shape-bell column:
new_data_df = data_df.drop(columns='cap-shape-bell');
new_data_df.columns[0] #the first attribute is different now
#also note that dataframe operations are immutable and you have to assign the output to a variable.

#Ok at this point I guess you know enough pandas to write the actual code.
#the main part that you gonna use dataframe is inside the build_tree function to split the data for each branch and
#in select_attr to do the required counting

#Here we have the neccessary data structure and auxilary functions for construcing the decision trees
#You don't need to modify the codes in this cell
class Node:
    """ Node class for a decision tree. """
    #var is the variable name for the node
    def __init__(self, var):
        self.var = var

    def classify(x):
        """ Handled by the subclasses. """
        return None

    def dump(self, indent):
        """ Handled by the subclasses. """
        return None


class Leaf(Node):
    #value is the label of the leaf node
    def __init__(self, value):
        Node.__init__(self, None);
        self.value = value

    def classify(self, x):
        return self.value

    def dump(self, indent):
        print(' %d' % self.value)


class Split(Node):
    #var: the variable that we create the split on
    #left and right are the branches for each side which are Nodes
    def __init__(self, var, left, right):
        Node.__init__(self, var)
        self.left = left
        self.right = right

    def classify(self, x):
        if x[self.var] == 0:
            return self.left.classify(x)
        else:
            return self.right.classify(x)

    #use to print out the tree recursively
    def dump(self, indent):
        if indent > 0:
            print('')
        for i in range(0, indent):
            print('| ', end='')
        print('%s = 0 :' % self.var,end='')
        self.left.dump(indent+1)
        for i in range(0, indent):
            print('| ', end='')
        print('%s = 1 :' % self.var,end='')
        self.right.dump(indent+1)

"""Helper function computes entropy of Bernoulli distribution with parameter p"""

def entropy(p):
  if p > 0 and p < 1:
    e = -(p * np.log2(p)) - ((1-p) * np.log2(1-p))
  else:
    e = 0

  return e;

"""Compute information gain for a particular split, given the counts

ny_nxi : number of occurences of y=1 with x_i=1 for all i=1 to n (#y=1 $\wedge$ #xi =1)
(n is number of variables and each variable is binary)

nxi : number of occurrences of x_i=1 (#x_i)

ny : number of ocurrences of y=1

total: total number instances in this branch

"""

def infogain(ny_nxi, nxi, ny, total):
    p_ofy = ny/total #probability of y=1
    p_ofxi = nxi/total #probability of xi=1
    p_ofny_nxi = ny_nxi/total   #joint probability of y=1 and xi=1
    p_ofny_gxi = p_ofny_nxi/p_ofxi #conditional probability of y given xi=1
    ny_nxiz = total - (total-ny) - ny_nxi #number of occurences where x=0 and y=1
    if total-nxi != 0:
      p_ofny_nxiz = ny_nxiz/(total-nxi) #conditional probability of y given x=0
    else:
       p_ofny_nxiz = 0
    i = entropy(p_ofy) - (p_ofxi*entropy(p_ofny_gxi) + (1-p_ofxi)*entropy(p_ofny_nxiz))

    return i;

#split_df: the portion of the dataframe that routed to the current branch
def select_attr(split_df):
    total = split_df.shape[0] #number of instances
    ny = (split_df[split_df.columns[-1]] == 1).sum() #number of positive classifications
    gain = -1
    selected_attr = None

    for att in split_df.columns: #iterting through attributes

      if att == split_df.columns[-1]:
        return (selected_attr, gain)

      ny_nxi = ((split_df[att] == 1) & (split_df[split_df.columns[-1]] == 1)).sum()  #number of occurences for xi=1 and y=1
      nxi = (split_df[att] == 1).sum() #number of occurences of xi = 1

      if ny_nxi == 0:
        ny_nxi = 0.001

      if nxi == 0:
        nxi = 0.001

      curr_gain = infogain(ny_nxi, nxi, ny, total)

      if curr_gain > gain:
        gain = curr_gain
        selected_attr = att

    return (selected_attr, gain);

"""Build tree in a top-down manner, selecting splits until we hit a pure leaf or all splits look bad.

"""

from IPython.core.inputtransformer2 import leading_empty_lines
from numpy.lib.function_base import select
#Recursive function for building the tree. Note that the vanilla ID3 stops when the nodes are pure (base condition for the recursion);
#config is potential dictionary of hyperparams that you can tune over validation dataset
#For potential list of hyperparams check here:
#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

def build_tree(data, config={}):

    if data.shape[1] == 1:  #base case when there are no attributes left to split over

        if (data[0] == 1).sum() > (data[0] == 0).sum():
          return Leaf(1)
        elif (data[0] == 1).sum() < (data[0] == 0).sum():
          return Leaf(0)
        else:
          return Leaf(np.random.randint(0, 2))

    elif (data[data.columns[-1]] == 1).sum() == data.shape[0]:  #base case when all the values of the class label = 1
        return Leaf(1)

    elif (data[data.columns[-1]] == 0).sum() == data.shape[0]: #base case when all the values of the class label = 0
        return Leaf(0)


    att = select_attr(data) #selecting attribute with highest information gain
    att_name = att[0]

    data_left = data[data[att_name] == 0]
    data_left = data_left.drop(columns=att_name) #subset of examples where attribute = 0

    data_right = data[data[att_name] == 1]
    data_right = data_right.drop(columns=att_name) #subset of examples where attribute = 1

    root = Split(att_name, build_tree(data_left), build_tree(data_right))

    return root;

"""Build the decision tree"""

model = build_tree(data_df)

model.dump(0)

"""Calcuating the accuracy"""

#Assume the last column in the class label and returns one dataframe for labels and one for attributes
def separate_attributes_label(data):
  class_column = data.columns[-1]
  xdata = data.drop(columns=class_column);
  ydata = data[class_column];
  return xdata, ydata

xtrain, ytrain = separate_attributes_label(data_df);

def accuracy(model, xdata, ydata):
  correct = 0.0;
  for i in range(xdata.shape[0]):
    if model.classify(xdata.loc[i]) == ydata.loc[i]:
      correct += 1;
  return correct / xdata.shape[0]

print("Train Accuracy: {}".format(accuracy(model, xtrain, ytrain)))



"""Now that we have all pieces working we can automate the training for any given dataset"""

def evaluate(dataset, config={}):
  train = pd.read_csv(path+dataset[0])
  val = pd.read_csv(path+dataset[1])
  test = pd.read_csv(path+dataset[2])

  xtrain, ytrain = separate_attributes_label(train);
  xval, yval = separate_attributes_label(val);
  xtest, ytest = separate_attributes_label(test);

  model = build_tree(train, config)
  model.dump(0)

  print("Train Accuracy: {}".format(accuracy(model, xtrain, ytrain)))
  print("Val Accuracy: {}".format(accuracy(model, xval, yval)))
  print("Test Accuracy: {}".format(accuracy(model, xtest, ytest)))

evaluate(agaricus)

config={'min_example': 2}
evaluate(dataset1, config)

